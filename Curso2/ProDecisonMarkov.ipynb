{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from itertools import product\n",
    "from random import choice\n",
    "class MDP:\n",
    "    \n",
    "    def __init__(self,s,r,a,T,gamma):\n",
    "        \"\"\"\n",
    "        Builds the MDP problem\n",
    "        :param s: states\n",
    "        :param r: rewards\n",
    "        :param a: actions\n",
    "        :param T: dictionary where keys are (s,a) pairs and\n",
    "        values are probabilities\n",
    "        :param gamma: the discount factor\n",
    "        \"\"\"\n",
    "        self.s = s\n",
    "        self.r = r\n",
    "        self.a = a\n",
    "        self.T = T\n",
    "        self.gamma = gamma\n",
    "        \n",
    "    def policy_iteration(self,pi=None):\n",
    "        \"\"\"\n",
    "            Policy iteration algorithm\n",
    "        \"\"\"\n",
    "        #initial random policy\n",
    "        if not pi:\n",
    "            pi = [choice(self.a) for s in self.s]\n",
    "        \n",
    "        print('pi = '+str(pi))\n",
    "        self.obtainT(pi);\n",
    "        T = self.obtainT(pi)\n",
    "        print('T(pi) = \\n'+str(T))\n",
    "        \n",
    "        V = np.matmul(np.linalg.inv(np.eye(3)-self.gamma*T.T),self.r)\n",
    "        print('V(s) = '+ str(V))\n",
    "        \n",
    "        pi_star = self.find_pi_star(V)\n",
    "        print(\"pi*(s) = \"+str(pi_star))\n",
    "        \n",
    "        if pi_star == pi:\n",
    "            return pi\n",
    "        else:\n",
    "            return self.policy_iteration(pi_star)\n",
    "        \n",
    "    def obtainT(self,pi):\n",
    "        \"\"\"\n",
    "        Obtains the transition probability matrix parametrized by the policy pi\n",
    "        :param pi: the policy\n",
    "        \"\"\"\n",
    "        return \\\n",
    "          np.matrix([[self.T[(s,t,pi[s])] for s in self.s] for t in self.s])\n",
    "        \n",
    "    def find_pi_star(self,V):\n",
    "        \"\"\"\n",
    "        Finds the optimal policy for the given infinite horizon values\n",
    "        :param V: the infinite horizon expected utility\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"\\n\".join([str((x,np.matmul(self.obtainT(x).T,np.array(V).T))) for x in product(self.a,self.a,self.a)]))\n",
    "        \n",
    "        return list(max(product(self.a,self.a,self.a),\\\n",
    "            key=lambda x: np.sum(np.matmul(self.obtainT(x).T,np.array(V).T))))\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2]\n"
     ]
    }
   ],
   "source": [
    "estados = [0,1,2]\n",
    "print(estados)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1]\n"
     ]
    }
   ],
   "source": [
    "acciones = [0,1]\n",
    "print(acciones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 10, 27]\n"
     ]
    }
   ],
   "source": [
    "recompensas = [0,10,27]\n",
    "print(recompensas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n"
     ]
    }
   ],
   "source": [
    "gamma = 0.9\n",
    "print(gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{(0, 0, 0): 0.7, (0, 0, 1): 0.5, (1, 0, 0): 0.4, (1, 0, 1): 0.2, (2, 0, 0): 0.2, (2, 0, 1): 0.1, (0, 1, 0): 0.1, (0, 1, 1): 0.3, (1, 1, 0): 0.4, (1, 1, 1): 0.7, (2, 1, 0): 0.2, (2, 1, 1): 0.1, (0, 2, 0): 0.2, (0, 2, 1): 0.2, (1, 2, 0): 0.2, (1, 2, 1): 0.1, (2, 2, 0): 0.6, (2, 2, 1): 0.8}\n"
     ]
    }
   ],
   "source": [
    "T={\n",
    "    (0,0,0):0.7,(0,0,1):0.5, (1,0,0):0.4,(1,0,1):0.2, (2,0,0):0.2,(2,0,1):0.1,\n",
    "    (0,1,0):0.1,(0,1,1):0.3, (1,1,0):0.4,(1,1,1):0.7, (2,1,0):0.2,(2,1,1):0.1,\n",
    "    (0,2,0):0.2,(0,2,1):0.2, (1,2,0):0.2,(1,2,1):0.1, (2,2,0):0.6,(2,2,1):0.8\n",
    "}\n",
    "print(T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdp = MDP(estados,recompensas,acciones,T,gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T(pi_0) = \n",
      "[[0.7 0.4 0.2]\n",
      " [0.1 0.4 0.2]\n",
      " [0.2 0.2 0.6]]\n"
     ]
    }
   ],
   "source": [
    "pi_0 = [0,0,0]\n",
    "print(\"T(pi_0) = \\n\"+str(mdp.obtainT(pi_0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pi = [0, 0, 0]\n",
      "T(pi) = \n",
      "[[0.7 0.4 0.2]\n",
      " [0.1 0.4 0.2]\n",
      " [0.2 0.2 0.6]]\n",
      "V(s) = [[ 91.73373288 105.43236301 135.84760274]]\n",
      "((0, 0, 0), matrix([[101.92636986],\n",
      "        [106.0359589 ],\n",
      "        [120.94178082]]))\n",
      "((0, 0, 1), matrix([[101.92636986],\n",
      "        [106.0359589 ],\n",
      "        [128.39469178]]))\n",
      "((0, 1, 0), matrix([[101.92636986],\n",
      "        [105.73416096],\n",
      "        [120.94178082]]))\n",
      "((0, 1, 1), matrix([[101.92636986],\n",
      "        [105.73416096],\n",
      "        [128.39469178]]))\n",
      "((1, 0, 0), matrix([[104.66609589],\n",
      "        [106.0359589 ],\n",
      "        [120.94178082]]))\n",
      "((1, 0, 1), matrix([[104.66609589],\n",
      "        [106.0359589 ],\n",
      "        [128.39469178]]))\n",
      "((1, 1, 0), matrix([[104.66609589],\n",
      "        [105.73416096],\n",
      "        [120.94178082]]))\n",
      "((1, 1, 1), matrix([[104.66609589],\n",
      "        [105.73416096],\n",
      "        [128.39469178]]))\n",
      "pi*(s) = [1, 0, 1]\n",
      "pi = [1, 0, 1]\n",
      "T(pi) = \n",
      "[[0.5 0.4 0.1]\n",
      " [0.3 0.4 0.1]\n",
      " [0.2 0.2 0.8]]\n",
      "V(s) = [[127.58241758 138.57142857 181.97802198]]\n",
      "((0, 0, 0), matrix([[139.56043956],\n",
      "        [142.85714286],\n",
      "        [162.41758242]]))\n",
      "((0, 0, 1), matrix([[139.56043956],\n",
      "        [142.85714286],\n",
      "        [172.1978022 ]]))\n",
      "((0, 1, 0), matrix([[139.56043956],\n",
      "        [140.71428571],\n",
      "        [162.41758242]]))\n",
      "((0, 1, 1), matrix([[139.56043956],\n",
      "        [140.71428571],\n",
      "        [172.1978022 ]]))\n",
      "((1, 0, 0), matrix([[141.75824176],\n",
      "        [142.85714286],\n",
      "        [162.41758242]]))\n",
      "((1, 0, 1), matrix([[141.75824176],\n",
      "        [142.85714286],\n",
      "        [172.1978022 ]]))\n",
      "((1, 1, 0), matrix([[141.75824176],\n",
      "        [140.71428571],\n",
      "        [162.41758242]]))\n",
      "((1, 1, 1), matrix([[141.75824176],\n",
      "        [140.71428571],\n",
      "        [172.1978022 ]]))\n",
      "pi*(s) = [1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "politica = mdp.policy_iteration(pi_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pi = [0, 1, 0]\n",
      "T(pi) = \n",
      "[[0.7 0.2 0.2]\n",
      " [0.1 0.7 0.2]\n",
      " [0.2 0.1 0.6]]\n",
      "V(s) = [[ 91.07107438 104.19504132 135.10413223]]\n",
      "((0, 0, 0), matrix([[101.19008264],\n",
      "        [105.12727273],\n",
      "        [120.11570248]]))\n",
      "((0, 0, 1), matrix([[101.19008264],\n",
      "        [105.12727273],\n",
      "        [127.60991736]]))\n",
      "((0, 1, 0), matrix([[101.19008264],\n",
      "        [104.66115702],\n",
      "        [120.11570248]]))\n",
      "((0, 1, 1), matrix([[101.19008264],\n",
      "        [104.66115702],\n",
      "        [127.60991736]]))\n",
      "((1, 0, 0), matrix([[103.81487603],\n",
      "        [105.12727273],\n",
      "        [120.11570248]]))\n",
      "((1, 0, 1), matrix([[103.81487603],\n",
      "        [105.12727273],\n",
      "        [127.60991736]]))\n",
      "((1, 1, 0), matrix([[103.81487603],\n",
      "        [104.66115702],\n",
      "        [120.11570248]]))\n",
      "((1, 1, 1), matrix([[103.81487603],\n",
      "        [104.66115702],\n",
      "        [127.60991736]]))\n",
      "pi*(s) = [1, 0, 1]\n",
      "pi = [1, 0, 1]\n",
      "T(pi) = \n",
      "[[0.5 0.4 0.1]\n",
      " [0.3 0.4 0.1]\n",
      " [0.2 0.2 0.8]]\n",
      "V(s) = [[127.58241758 138.57142857 181.97802198]]\n",
      "((0, 0, 0), matrix([[139.56043956],\n",
      "        [142.85714286],\n",
      "        [162.41758242]]))\n",
      "((0, 0, 1), matrix([[139.56043956],\n",
      "        [142.85714286],\n",
      "        [172.1978022 ]]))\n",
      "((0, 1, 0), matrix([[139.56043956],\n",
      "        [140.71428571],\n",
      "        [162.41758242]]))\n",
      "((0, 1, 1), matrix([[139.56043956],\n",
      "        [140.71428571],\n",
      "        [172.1978022 ]]))\n",
      "((1, 0, 0), matrix([[141.75824176],\n",
      "        [142.85714286],\n",
      "        [162.41758242]]))\n",
      "((1, 0, 1), matrix([[141.75824176],\n",
      "        [142.85714286],\n",
      "        [172.1978022 ]]))\n",
      "((1, 1, 0), matrix([[141.75824176],\n",
      "        [140.71428571],\n",
      "        [162.41758242]]))\n",
      "((1, 1, 1), matrix([[141.75824176],\n",
      "        [140.71428571],\n",
      "        [172.1978022 ]]))\n",
      "pi*(s) = [1, 0, 1]\n"
     ]
    }
   ],
   "source": [
    "#cambia las recompensas y ejecuta la siguiente línea. \n",
    "politica = mdp.policy_iteration([0,1,0])\n",
    "\n",
    "#copia el resultado en un archivo y sométe la respuesta."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
